[2025-05-21 23:58:55,144] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-21 23:58:55,144] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-21 23:58:57,139] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-05-21 23:58:57,139] [INFO] [comm.py:652:init_distributed] cdb=None
[2025-05-21 23:58:57,140] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
ninja: no work to do.
Time to load fused_adam op: 0.692408561706543 seconds
Time to load fused_adam op: 0.7394812107086182 seconds
{'loss': 7.3987, 'grad_norm': 16.909154891967773, 'learning_rate': 3.750846785039731e-07, 'epoch': 0.0}
{'loss': 7.341, 'grad_norm': 15.63774299621582, 'learning_rate': 4.789655177777303e-07, 'epoch': 0.01}
{'loss': 7.3222, 'grad_norm': 11.730945587158203, 'learning_rate': 5.397319132963199e-07, 'epoch': 0.01}
{'loss': 7.2107, 'grad_norm': 8.2770414352417, 'learning_rate': 5.828463570514876e-07, 'epoch': 0.02}
{'loss': 7.158, 'grad_norm': 7.718080043792725, 'learning_rate': 6.162885177341887e-07, 'epoch': 0.02}
{'loss': 7.1141, 'grad_norm': 7.120306015014648, 'learning_rate': 6.436127525700772e-07, 'epoch': 0.03}
{'loss': 7.078, 'grad_norm': 6.201200485229492, 'learning_rate': 6.667150639466306e-07, 'epoch': 0.03}
{'loss': 7.0455, 'grad_norm': 5.948206424713135, 'learning_rate': 6.867271963252449e-07, 'epoch': 0.04}
{'loss': 7.0048, 'grad_norm': 5.760951042175293, 'learning_rate': 7.043791480886668e-07, 'epoch': 0.04}
{'loss': 6.9685, 'grad_norm': 5.337438106536865, 'learning_rate': 7.201693570079461e-07, 'epoch': 0.05}
{'loss': 6.936, 'grad_norm': 5.606473922729492, 'learning_rate': 7.344533384581882e-07, 'epoch': 0.05}
{'loss': 6.922, 'grad_norm': 5.038304805755615, 'learning_rate': 7.474935918438345e-07, 'epoch': 0.06}
{'loss': 6.9029, 'grad_norm': 5.110324382781982, 'learning_rate': 7.594894621064156e-07, 'epoch': 0.06}
{'loss': 6.9008, 'grad_norm': 5.144055366516113, 'learning_rate': 7.705959032203877e-07, 'epoch': 0.06}
{'loss': 6.8684, 'grad_norm': 5.322976112365723, 'learning_rate': 7.809357525265356e-07, 'epoch': 0.07}
{'loss': 6.8166, 'grad_norm': 4.943465232849121, 'learning_rate': 7.906080355990022e-07, 'epoch': 0.07}
{'loss': 6.8172, 'grad_norm': 5.324559211730957, 'learning_rate': 7.996937489533551e-07, 'epoch': 0.08}
{'loss': 6.8158, 'grad_norm': 5.451879501342773, 'learning_rate': 8.082599873624241e-07, 'epoch': 0.08}
{'loss': 6.7756, 'grad_norm': 4.8591437339782715, 'learning_rate': 8.163629537745778e-07, 'epoch': 0.09}
{'loss': 6.7627, 'grad_norm': 5.142679214477539, 'learning_rate': 8.240501962817033e-07, 'epoch': 0.09}
{'eval_loss': 6.754260540008545, 'eval_runtime': 24.6488, 'eval_samples_per_second': 40.57, 'eval_steps_per_second': 5.071, 'epoch': 0.09}
{'loss': 6.798, 'grad_norm': 4.967453956604004, 'learning_rate': 8.313622987389774e-07, 'epoch': 0.1}
{'loss': 6.7526, 'grad_norm': 5.338945388793945, 'learning_rate': 8.383341777319456e-07, 'epoch': 0.1}
{'loss': 6.7057, 'grad_norm': 5.489495277404785, 'learning_rate': 8.449960910060149e-07, 'epoch': 0.11}
{'loss': 6.7146, 'grad_norm': 4.888456344604492, 'learning_rate': 8.513744311175919e-07, 'epoch': 0.11}
{'loss': 6.7134, 'grad_norm': 5.360899448394775, 'learning_rate': 8.574923569644041e-07, 'epoch': 0.12}
{'loss': 6.6939, 'grad_norm': 5.241118431091309, 'learning_rate': 8.63370301380173e-07, 'epoch': 0.12}
{'loss': 6.6668, 'grad_norm': 5.790677070617676, 'learning_rate': 8.690263828810137e-07, 'epoch': 0.13}
{'loss': 6.7007, 'grad_norm': 5.527853488922119, 'learning_rate': 8.744767424941451e-07, 'epoch': 0.13}
{'loss': 6.6652, 'grad_norm': 5.221424579620361, 'learning_rate': 8.79735821453788e-07, 'epoch': 0.13}
{'loss': 6.6551, 'grad_norm': 5.010112285614014, 'learning_rate': 8.848165918002928e-07, 'epoch': 0.14}
{'loss': 6.6508, 'grad_norm': 5.411380767822266, 'learning_rate': 8.897307491539136e-07, 'epoch': 0.14}
{'loss': 6.6764, 'grad_norm': 5.426746368408203, 'learning_rate': 8.944888748727596e-07, 'epoch': 0.15}
{'loss': 6.6541, 'grad_norm': 5.407516002655029, 'learning_rate': 8.991005732505352e-07, 'epoch': 0.15}
{'loss': 6.6266, 'grad_norm': 5.254611015319824, 'learning_rate': 9.035745882271122e-07, 'epoch': 0.16}
{'loss': 6.5956, 'grad_norm': 5.9089226722717285, 'learning_rate': 9.079189031768461e-07, 'epoch': 0.16}
{'loss': 6.597, 'grad_norm': 5.355569839477539, 'learning_rate': 9.121408266361816e-07, 'epoch': 0.17}
{'loss': 6.5903, 'grad_norm': 5.287220001220703, 'learning_rate': 9.162470662830081e-07, 'epoch': 0.17}
{'loss': 6.6073, 'grad_norm': 5.461008548736572, 'learning_rate': 9.20243793048335e-07, 'epoch': 0.18}
{'loss': 6.5921, 'grad_norm': 5.422530651092529, 'learning_rate': 9.241366968987626e-07, 'epoch': 0.18}
{'loss': 6.5766, 'grad_norm': 5.58998441696167, 'learning_rate': 9.279310355554606e-07, 'epoch': 0.19}
{'eval_loss': 6.581997394561768, 'eval_runtime': 24.633, 'eval_samples_per_second': 40.596, 'eval_steps_per_second': 5.075, 'epoch': 0.19}
{'loss': 6.574, 'grad_norm': 5.575180530548096, 'learning_rate': 9.316316771965006e-07, 'epoch': 0.19}
{'loss': 6.6149, 'grad_norm': 5.40877103805542, 'learning_rate': 9.352431380127347e-07, 'epoch': 0.19}
[1;34mwandb[0m: ðŸš€ View run [33m/mnt/fast/nobackup/scratch4weeks/yc01815/llasa/LLaSA_training/finetune/EVC_0521[0m at: [34mhttps://wandb.ai/cheny2707/huggingface/runs/bnrkqcnd[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250522_000119-bnrkqcnd/logs[0m
