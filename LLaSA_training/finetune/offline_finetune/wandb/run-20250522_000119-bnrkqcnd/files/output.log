  0%|          | 0/21560 [00:00<?, ?it/s]/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
                                                      
{'loss': 7.3987, 'grad_norm': 16.909154891967773, 'learning_rate': 3.750846785039731e-07, 'epoch': 0.0}
{'loss': 7.341, 'grad_norm': 15.63774299621582, 'learning_rate': 4.789655177777303e-07, 'epoch': 0.01}
{'loss': 7.3222, 'grad_norm': 11.730945587158203, 'learning_rate': 5.397319132963199e-07, 'epoch': 0.01}
{'loss': 7.2107, 'grad_norm': 8.2770414352417, 'learning_rate': 5.828463570514876e-07, 'epoch': 0.02}
{'loss': 7.158, 'grad_norm': 7.718080043792725, 'learning_rate': 6.162885177341887e-07, 'epoch': 0.02}
{'loss': 7.1141, 'grad_norm': 7.120306015014648, 'learning_rate': 6.436127525700772e-07, 'epoch': 0.03}
{'loss': 7.078, 'grad_norm': 6.201200485229492, 'learning_rate': 6.667150639466306e-07, 'epoch': 0.03}
{'loss': 7.0455, 'grad_norm': 5.948206424713135, 'learning_rate': 6.867271963252449e-07, 'epoch': 0.04}
{'loss': 7.0048, 'grad_norm': 5.760951042175293, 'learning_rate': 7.043791480886668e-07, 'epoch': 0.04}
{'loss': 6.9685, 'grad_norm': 5.337438106536865, 'learning_rate': 7.201693570079461e-07, 'epoch': 0.05}
{'loss': 6.936, 'grad_norm': 5.606473922729492, 'learning_rate': 7.344533384581882e-07, 'epoch': 0.05}
{'loss': 6.922, 'grad_norm': 5.038304805755615, 'learning_rate': 7.474935918438345e-07, 'epoch': 0.06}
{'loss': 6.9029, 'grad_norm': 5.110324382781982, 'learning_rate': 7.594894621064156e-07, 'epoch': 0.06}
{'loss': 6.9008, 'grad_norm': 5.144055366516113, 'learning_rate': 7.705959032203877e-07, 'epoch': 0.06}
{'loss': 6.8684, 'grad_norm': 5.322976112365723, 'learning_rate': 7.809357525265356e-07, 'epoch': 0.07}
{'loss': 6.8166, 'grad_norm': 4.943465232849121, 'learning_rate': 7.906080355990022e-07, 'epoch': 0.07}
{'loss': 6.8172, 'grad_norm': 5.324559211730957, 'learning_rate': 7.996937489533551e-07, 'epoch': 0.08}
{'loss': 6.8158, 'grad_norm': 5.451879501342773, 'learning_rate': 8.082599873624241e-07, 'epoch': 0.08}
{'loss': 6.7756, 'grad_norm': 4.8591437339782715, 'learning_rate': 8.163629537745778e-07, 'epoch': 0.09}
{'loss': 6.7627, 'grad_norm': 5.142679214477539, 'learning_rate': 8.240501962817033e-07, 'epoch': 0.09}
  File "/mnt/fast/nobackup/users/yc01815/code/llasa/LLaSA_training/finetune/offline_finetune/finetune_offline.py", line 243, in <module>
{'eval_loss': 6.754260540008545, 'eval_runtime': 24.6488, 'eval_samples_per_second': 40.57, 'eval_steps_per_second': 5.071, 'epoch': 0.09}
{'loss': 6.798, 'grad_norm': 4.967453956604004, 'learning_rate': 8.313622987389774e-07, 'epoch': 0.1}
{'loss': 6.7526, 'grad_norm': 5.338945388793945, 'learning_rate': 8.383341777319456e-07, 'epoch': 0.1}
{'loss': 6.7057, 'grad_norm': 5.489495277404785, 'learning_rate': 8.449960910060149e-07, 'epoch': 0.11}
{'loss': 6.7146, 'grad_norm': 4.888456344604492, 'learning_rate': 8.513744311175919e-07, 'epoch': 0.11}
{'loss': 6.7134, 'grad_norm': 5.360899448394775, 'learning_rate': 8.574923569644041e-07, 'epoch': 0.12}
{'loss': 6.6939, 'grad_norm': 5.241118431091309, 'learning_rate': 8.63370301380173e-07, 'epoch': 0.12}
{'loss': 6.6668, 'grad_norm': 5.790677070617676, 'learning_rate': 8.690263828810137e-07, 'epoch': 0.13}
{'loss': 6.7007, 'grad_norm': 5.527853488922119, 'learning_rate': 8.744767424941451e-07, 'epoch': 0.13}
{'loss': 6.6652, 'grad_norm': 5.221424579620361, 'learning_rate': 8.79735821453788e-07, 'epoch': 0.13}
{'loss': 6.6551, 'grad_norm': 5.010112285614014, 'learning_rate': 8.848165918002928e-07, 'epoch': 0.14}
{'loss': 6.6508, 'grad_norm': 5.411380767822266, 'learning_rate': 8.897307491539136e-07, 'epoch': 0.14}
{'loss': 6.6764, 'grad_norm': 5.426746368408203, 'learning_rate': 8.944888748727596e-07, 'epoch': 0.15}
{'loss': 6.6541, 'grad_norm': 5.407516002655029, 'learning_rate': 8.991005732505352e-07, 'epoch': 0.15}
{'loss': 6.6266, 'grad_norm': 5.254611015319824, 'learning_rate': 9.035745882271122e-07, 'epoch': 0.16}
{'loss': 6.5956, 'grad_norm': 5.9089226722717285, 'learning_rate': 9.079189031768461e-07, 'epoch': 0.16}
{'loss': 6.597, 'grad_norm': 5.355569839477539, 'learning_rate': 9.121408266361816e-07, 'epoch': 0.17}
{'loss': 6.5903, 'grad_norm': 5.287220001220703, 'learning_rate': 9.162470662830081e-07, 'epoch': 0.17}
{'loss': 6.6073, 'grad_norm': 5.461008548736572, 'learning_rate': 9.20243793048335e-07, 'epoch': 0.18}
{'loss': 6.5921, 'grad_norm': 5.422530651092529, 'learning_rate': 9.241366968987626e-07, 'epoch': 0.18}
{'loss': 6.5766, 'grad_norm': 5.58998441696167, 'learning_rate': 9.279310355554606e-07, 'epoch': 0.19}
{'eval_loss': 6.581997394561768, 'eval_runtime': 24.633, 'eval_samples_per_second': 40.596, 'eval_steps_per_second': 5.075, 'epoch': 0.19}
{'loss': 6.574, 'grad_norm': 5.575180530548096, 'learning_rate': 9.316316771965006e-07, 'epoch': 0.19}
{'loss': 6.6149, 'grad_norm': 5.40877103805542, 'learning_rate': 9.352431380127347e-07, 'epoch': 0.19}
    main()
  File "/mnt/fast/nobackup/users/yc01815/code/llasa/LLaSA_training/finetune/offline_finetune/finetune_offline.py", line 235, in main
    trainer.train()
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 2052, in train
    return inner_training_loop(
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 3518, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/accelerate/accelerator.py", line 2233, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
    self.engine.backward(loss, **kwargs)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2064, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/code/llasa/LLaSA_training/finetune/offline_finetune/finetune_offline.py", line 243, in <module>
[rank0]:     main()
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/code/llasa/LLaSA_training/finetune/offline_finetune/finetune_offline.py", line 235, in main
[rank0]:     trainer.train()
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 2052, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 2388, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/transformers/trainer.py", line 3518, in training_step
[rank0]:     self.accelerator.backward(loss, **kwargs)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/accelerate/accelerator.py", line 2233, in backward
[rank0]:     self.deepspeed_engine_wrapped.backward(loss, **kwargs)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/accelerate/utils/deepspeed.py", line 186, in backward
[rank0]:     self.engine.backward(loss, **kwargs)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/utils/nvtx.py", line 18, in wrapped_fn
[rank0]:     ret_val = func(*args, **kwargs)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/engine.py", line 2020, in backward
[rank0]:     self.optimizer.backward(loss, retain_graph=retain_graph)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2064, in backward
[rank0]:     self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
[rank0]:     scaled_loss.backward(retain_graph=retain_graph)
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/_tensor.py", line 521, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/autograd/__init__.py", line 289, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/mnt/fast/nobackup/users/yc01815/anaconda3/envs/llasa/lib/python3.9/site-packages/torch/autograd/graph.py", line 769, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]: KeyboardInterrupt
